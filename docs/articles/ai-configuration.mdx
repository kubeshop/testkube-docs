# Testkube AI Configuration Reference

This document provides a comprehensive reference for enabling and configuring Testkube AI functionality, including [AI Assistant](./ai-assistant-overview), [AI Agents](./ai-agents), and related features.

## Cloud Users

For Testkube Cloud users, enabling AI functionality is straightforward:

### Enable AI Assistant

AI Assistant is disabled by default for new organizations.

**Who can enable:** Organization Owner or Admin

**How to enable:**
1. Navigate to **Organization Settings** → **Product Features** tab
2. Toggle **AI Assistant** on

That's it! No additional configuration, databases, or API keys are needed — everything is managed by Testkube. The AI functionality uses OpenAI GPT-5.2-Codex by default (the model cannot be changed in cloud installations).

---

## On-Prem Users

Self-hosted installations require additional infrastructure and configuration.

### Infrastructure Requirements

#### PostgreSQL Database

A PostgreSQL database is required for LangGraph checkpointing (conversation persistence). The AI service creates and uses the following tables:

| Table | Purpose |
|-------|---------|
| `ai_mcp_servers` | MCP server configurations |
| `ai_assistants` | AI assistant definitions |
| `ai_assistant_sessions` | Session executions and history |
| `ai_mcp_server_tools` | Tools provided by MCP servers |
| `ai_session_tool_calls` | Tool invocations within sessions |

Configure the database connection via `POSTGRES_URI` environment variable or Helm `db.secretRef` configuration.

#### NATS Message Queue

The AI service uses the global NATS instance for async operations. No additional NATS configuration is required if you already have Testkube deployed.

### Helm Configuration

Configure the following components in your `testkube-enterprise` Helm values:

```yaml
# Enable the AI service
testkube-ai-service:
  enabled: true  # Default: false
  llmApi:
    url: ""  # Optional - defaults to OpenAI API when omitted
    secretRef: "<secret-name>"  # K8s secret storing LLM_API_KEY
    secretRefKey: "LLM_API_KEY"  # Default key name

# Enable AI features in the UI
testkube-cloud-ui:
  ai:
    enabled: true  # Default: false
    aiServiceApiUri: "https://ai.<your-domain>"
```

### LLM API Key Setup

Create a Kubernetes secret containing your LLM API key:

```bash
kubectl -n <namespace> create secret generic <secret-name> \
  --from-literal=LLM_API_KEY=<your-api-key>
```

:::tip
The secret must be in the same namespace as the Testkube control plane.
:::

### LLM Provider Options

Testkube supports any LLM service that implements the OpenAI API specification:

#### OpenAI (Direct)

Leave `url` empty and provide the secret reference:

```yaml
testkube-ai-service:
  enabled: true
  llmApi:
    secretRef: "<secret-name>"
```

#### Self-Hosted or Third-Party LLMs

For self-hosted models (vLLM, LiteLLM, OpenLLM) or commercial gateways:

```yaml
testkube-ai-service:
  enabled: true
  llmApi:
    url: "http://your-llm-service:8000/v1"
    secretRef: "<secret-name>"
```

#### Testkube Hosted LLM Proxy (Trials Only)

For evaluation purposes, you can use the Testkube hosted proxy:

```yaml
testkube-ai-service:
  enabled: true
  llmApi:
    url: "https://llm.testkube.io"
    # No secretRef needed - authentication handled via license key
```

:::warning
The hosted proxy is intended **only for trials, demos, and onboarding**. It has usage limits and is not recommended for production workloads.
:::

### Authentication Configuration

The AI service requires OAuth/OIDC authentication. Configure one of the following:

| Option | Environment Variable | Description |
|--------|---------------------|-------------|
| OIDC Discovery (recommended) | `OIDC_CONFIGURATION_URL` | Auto-discovers endpoints from Dex |
| Manual configuration | `OAUTH_JWKS_URL` + `OAUTH_ISSUER` | Provide both URLs manually |

When using Dex (default), authentication is automatically configured via the global Dex issuer settings.

### Network Requirements

Ensure your firewall allows traffic to:

- **LLM endpoint** — OpenAI API or your custom LLM service
- **Testkube AI Service API** — The deployed AI service endpoint

For corporate proxies, inject proxy variables into the AI service pod:

```yaml
testkube-ai-service:
  enabled: true
  llmApi:
    secretRef: "<secret-name>"
  extraEnvVars:
    - name: HTTP_PROXY
      value: "http://proxy.domain:8080"
    - name: HTTPS_PROXY
      value: "https://proxy.domain:8043"
    - name: NO_PROXY
      value: ""
```

---

## Feature Flags Reference

| Flag/Setting | Location | Default | Purpose |
|-------------|----------|---------|---------|
| `testkube-ai-service.enabled` | Helm | `false` | Deploy AI service |
| `testkube-cloud-ui.ai.enabled` | Helm | `false` | Enable UI AI features |
| `MCP_ENABLED` | Control Plane env | `false` | Enable MCP integration |
| `aiCopilotEnabled` | User settings (API) | `false` | Per-user AI toggle |

---

## Environment Variables Reference

The following environment variables can be configured for the AI service:

### Required Variables

| Variable | Description |
|----------|-------------|
| `CONTROL_PLANE_ENDPOINT` | URL endpoint for the Testkube Control Plane |
| `POSTGRES_URI` | PostgreSQL connection string for checkpointing |

### LLM Configuration

| Variable | Required | Description |
|----------|----------|-------------|
| `LLM_API_KEY` | Conditional | Required if not using license key or custom LLM |
| `LLM_API_URL` | No | Custom LLM endpoint (OpenAI-compatible) |
| `MODEL_NAME` | No | Model name (e.g., `gpt-4o`, `gpt-5.2-codex`) |

### Authentication

| Variable | Required | Description |
|----------|----------|-------------|
| `OIDC_CONFIGURATION_URL` | Conditional | OIDC discovery URL (recommended) |
| `OAUTH_JWKS_URL` | Conditional | JWK set document URL (if not using OIDC discovery) |
| `OAUTH_ISSUER` | Conditional | OAuth issuer URL (if not using OIDC discovery) |

### Optional Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `NODE_ENV` | `development` | Runtime environment |
| `LOG_LEVEL` | `info` | Logging verbosity |
| `PORT` | `9090` | Server port |
| `USE_TLS` | — | Enable/disable TLS |
| `TLS_CERT` | — | TLS certificate file path |
| `TLS_KEY` | — | TLS private key file path |
| `GRAPH_RECURSION_LIMIT` | `100` | Maximum recursion depth for LangGraph agent |

---

## Quick Start Checklist

### Cloud Users

- [ ] Enable AI Assistant in Organization Settings → Product Features

### On-Prem Users

- [ ] PostgreSQL database available and accessible
- [ ] LLM API key secret created in the correct namespace
- [ ] Helm values configured:
  - [ ] `testkube-ai-service.enabled: true`
  - [ ] `testkube-ai-service.llmApi.secretRef` set
  - [ ] `testkube-cloud-ui.ai.enabled: true`
  - [ ] `testkube-cloud-ui.ai.aiServiceApiUri` set
- [ ] Network access to LLM endpoint configured
- [ ] OAuth/OIDC authentication configured (or using default Dex)
- [ ] Enable AI Assistant in Organization Settings after deployment

---

### External Secrets

If using External Secrets Operator:

```yaml
testkube-ai-service:
  externalSecrets:
    enabled: true
    refreshInterval: 5m
    clusterSecretStoreName: secret-store
```

## See Also

- [AI Agents](./ai-agents)
- [AI Assistant Overview](./ai-assistant-overview)
- [AI Assistant On-Prem Installation](./ai-assistant-on-prem-install)
- [AI Assistant Enablement](./ai-assistant-enable)
- [AI Assistant Security & Compliance](./ai-assistant-security)
